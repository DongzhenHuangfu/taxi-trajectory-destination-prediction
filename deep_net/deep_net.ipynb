{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the labrary and define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from  sklearn.utils import shuffle\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function to translate the string data into a float list\n",
    "\n",
    "def str2array(string):    \n",
    "    together = ''\n",
    "    for i in string:\n",
    "        if(i != '['):\n",
    "            together += i\n",
    "    \n",
    "    first = together.split(\"],\")\n",
    "    ret = []\n",
    "    \n",
    "    for i in range(len(first)):\n",
    "        if(i==len(first)-1):\n",
    "            second = ''\n",
    "            for j in first[i]:\n",
    "                if(j!=']'):\n",
    "                    second += j\n",
    "            second = second.split(\",\")\n",
    "        else:\n",
    "            second = first[i].split(\",\")\n",
    "        ret.append([float(second[0]), float(second[1])])\n",
    "    return ret\n",
    "\n",
    "## define a function for exploring the maximum and minimum length of the track data\n",
    "\n",
    "def find_maxmin(data):\n",
    "    maximum = 0\n",
    "    minimum = 99999\n",
    "    for track in data:\n",
    "        if(len(track) > maximum):\n",
    "            maximum = len(track)\n",
    "        if(len(track) < minimum):\n",
    "            minimum = len(track)\n",
    "    return maximum, minimum\n",
    "\n",
    "## def a functino for exploring the distribution of the track length\n",
    "\n",
    "def count(data, number = 10, maximum = 4000):\n",
    "    count_num = []\n",
    "    part = maximum//number\n",
    "    level = part*np.array(range(1+number))\n",
    "    print(\"level is:\", level)\n",
    "    \n",
    "    for i in range(number):\n",
    "        count_num.append(0)\n",
    "    \n",
    "    for line in data:\n",
    "        for i in range(len(level)-1):\n",
    "            if((len(line) < level[i+1]) & (len(line) >= level[i])):\n",
    "                count_num[i] += 1\n",
    "                continue\n",
    "    return count_num\n",
    "\n",
    "## define a function for exploring the number of the track which is above a value\n",
    "\n",
    "def above(data, level):\n",
    "    count = 0\n",
    "    for line in data:\n",
    "        if(len(line)>=level):\n",
    "            count += 1            \n",
    "    return count\n",
    "\n",
    "## define a function for selecting track data\n",
    "\n",
    "def select_data(track, place):\n",
    "    ret = []\n",
    "    for i in place:\n",
    "        ret.append(track[i])\n",
    "    return ret\n",
    "\n",
    "def pick_data(data, data_length):\n",
    "    ret_train = []\n",
    "    ret_label = []\n",
    "    n = 0\n",
    "    for i in range(len(data)):\n",
    "        if(len(data[i]) > data_length):\n",
    "            split = len(data[i])//data_length\n",
    "            ret_train.append([])\n",
    "            for j in range(data_length):\n",
    "                ret_train[n].append(data[i][split*j])\n",
    "            ret_label.append(data[i][len(data[i])-1])\n",
    "            n += 1\n",
    "    return ret_train, ret_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../train_data.pickle', 'rb') as file:\n",
    "    data_dict = pickle.load(file)\n",
    "    train_data = data_dict['train_data10']\n",
    "    train_label = data_dict['train_label10']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])\n",
    "print()\n",
    "print(train_label[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the area into n parts, only predict the area for the destination\n",
    "max_x = -99999.;\n",
    "max_y = -99999.;\n",
    "min_x = 99999.;\n",
    "min_y = 99999.;\n",
    "\n",
    "for point in train_label:\n",
    "    if max_x < point[0]: max_x = point[0]\n",
    "    if max_y < point[1]: max_y = point[1]\n",
    "    if min_x > point[0]: min_x = point[0]\n",
    "    if min_y > point[1]: min_y = point[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_x, min_y, max_x, max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_len = 0.16\n",
    "area = []\n",
    "labels = []\n",
    "x = min_x\n",
    "y = min_y\n",
    "i = 0\n",
    "while(x < max_x + div_len):\n",
    "    y = min_y\n",
    "    while(y < max_y + div_len):\n",
    "        area.append([x, y])\n",
    "        y += div_len\n",
    "    i += 1\n",
    "    x += div_len\n",
    "    \n",
    "print(\"Together %d areas\" % (len(area) * len(area[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(area[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for point in train_label:\n",
    "    find = False    \n",
    "    for i in range(len(area) - 1):\n",
    "        if((point[0] < area[i][0]) & (point[0] < area[i][1])):\n",
    "            find = True \n",
    "            labels.append(i)\n",
    "            break\n",
    "    if not find:\n",
    "        print(\"Error at i = %d\" % i)\n",
    "        print(\"Point:\", point)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels), len(train_data), len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count how many times a label shows up\n",
    "count = []\n",
    "showup = []\n",
    "for index in labels:\n",
    "    add = True\n",
    "    for i in range(len(showup)):\n",
    "        if(showup[i] == index):\n",
    "            add = False\n",
    "            count[i] += 1\n",
    "    if(add): \n",
    "        showup.append(index)\n",
    "        count.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(showup)):\n",
    "    print(\"{0:<5}\".format(showup[i]), ':', count[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_data, labels, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the data into a pickle file\n",
    "# need to save: data, area, labels_for_train\n",
    "data_dict = {'x_train': x_train, 'x_test': x_test, 'y_train': y_train, 'y_test': y_test, \"area\": area,\n",
    "            \"labels\": labels}\n",
    "with open('./data/train_for_deepstructure_area.pickle','wb') as file:\n",
    "    pickle.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_for_deepstructure_area.pickle', 'rb') as file:\n",
    "    data_dict = pickle.load(file)\n",
    "    x_train = data_dict['x_train']\n",
    "    x_test = data_dict['x_test']    \n",
    "    y_train = data_dict['y_train']\n",
    "    y_test = data_dict['y_test']\n",
    "    area = data_dict['area']\n",
    "    labels = data_dict['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an easy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,(None, 20))\n",
    "\n",
    "# Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "# Layer 1: Input = 20. Output = 3600.\n",
    "fc1_W = tf.Variable(tf.truncated_normal(shape=(20, 1000), mean = mu, stddev = sigma))\n",
    "fc1_b = tf.Variable(tf.zeros(1000))\n",
    "fc1_lin   = tf.matmul(x, fc1_W) + fc1_b\n",
    "\n",
    "# Activation.\n",
    "fc1 = tf.nn.relu(fc1_lin)\n",
    "\n",
    "# Layer 2: Input = 1000. Output = 3600.\n",
    "fc2_W = tf.Variable(tf.truncated_normal(shape=(1000, 3600), mean = mu, stddev = sigma))\n",
    "fc2_b = tf.Variable(tf.zeros(3600))\n",
    "logits   = tf.matmul(fc1, fc2_W) + fc2_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32,(None))\n",
    "one_hot_y = tf.one_hot(y,3600)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = one_hot_y, logits = logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow parameters\n",
    "epoches = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation accuracy\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "# when the accuracy is high, I want to make the learning rate smaller.\n",
    "n_train = len(x_train)\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training ......')\n",
    "        print()\n",
    "        max_acc = 0\n",
    "\n",
    "        for i in range(epoches):\n",
    "            X_train,y_train = shuffle(x_train,y_train) # shuffle the data\n",
    "            for offset in range(0,n_train,batch_size):\n",
    "                end = offset + batch_size\n",
    "                batch_x, batch_y = X_train[offset:end],y_train[offset:end]\n",
    "                sess.run(training_operation,feed_dict = {x:batch_x, y:batch_y})\n",
    "\n",
    "            validation_accuracy = evaluate(x_test, y_test)\n",
    "            if(max_acc <= validation_accuracy):\n",
    "                max_acc = validation_accuracy\n",
    "                saver.save(sess, './result')\n",
    "                print(\"Model saved with accuracy:\", max_acc)\n",
    "            if i%5 == 0:\n",
    "                print(\"EPOCH {} ...\".format(i+1))\n",
    "                print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "    #             print('Can be better, continue training...')\n",
    "                print()\n",
    "\n",
    "        print(\"maximal accuracy is:\", max_acc)\n",
    "        print(\"End training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only 1133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../trackdata1133.pickle', 'rb') as file:\n",
    "    data_dict = pickle.load(file)\n",
    "    x_train = data_dict['x_train']\n",
    "    x_test = data_dict['x_test']\n",
    "    y_train = data_dict['y_train']\n",
    "    y_test = data_dict['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,(None, 20))\n",
    "\n",
    "# Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "# Layer 1: Input = 20. Output = 100.\n",
    "fc1_W = tf.Variable(tf.truncated_normal(shape=(20, 100), mean = mu, stddev = sigma))\n",
    "fc1_b = tf.Variable(tf.zeros(100))\n",
    "fc1_lin   = tf.matmul(x, fc1_W) + fc1_b\n",
    "\n",
    "# Activation.\n",
    "fc1 = tf.nn.relu(fc1_lin)\n",
    "\n",
    "# Layer 2: Input = 100. Output = 900.\n",
    "fc2_W = tf.Variable(tf.truncated_normal(shape=(100, 900), mean = mu, stddev = sigma))\n",
    "fc2_b = tf.Variable(tf.zeros(900))\n",
    "logits   = tf.matmul(fc1, fc2_W) + fc2_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32,(None))\n",
    "one_hot_y = tf.one_hot(y,900)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = one_hot_y, logits = y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow parameters\n",
    "epoches = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation accuracy\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(x_train)\n",
    "## with tf.device('/cpu:0'):\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training ......')\n",
    "    print()\n",
    "    max_acc = 0\n",
    "\n",
    "    for i in range(epoches):\n",
    "        X_train,y_train = shuffle(x_train,y_train) # shuffle the data\n",
    "        for offset in range(0,n_train,batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_train[offset:end],y_train[offset:end]\n",
    "            sess.run(training_operation,feed_dict = {x:batch_x, y:batch_y})\n",
    "\n",
    "        validation_accuracy = evaluate(x_test, y_test)\n",
    "        if(max_acc <= validation_accuracy):\n",
    "            max_acc = validation_accuracy\n",
    "            saver.save(sess, './result')\n",
    "            print(\"Model saved with accuracy:\", max_acc)\n",
    "        if i%5 == 0:\n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "#             print('Can be better, continue training...')\n",
    "            print()\n",
    "\n",
    "    print(\"maximal accuracy is:\", max_acc)\n",
    "    print(\"End training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rnn method\n",
    "\n",
    "## rnn parameters\n",
    "n_inputs = 20\n",
    "n_steps = 1\n",
    "n_hidden = 900\n",
    "n_classes = 900\n",
    "\n",
    "x = tf.placeholder(tf.float32,(None, 20))\n",
    "y = tf.placeholder(tf.int32, None)\n",
    "size = tf.placeholder(tf.int32)\n",
    "one_hot_y = tf.one_hot(y,900)\n",
    "\n",
    "weights = {\n",
    "    \n",
    "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden])),\n",
    "    \n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden, ])),\n",
    "    \n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow parameters\n",
    "epoches = 1000\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(X, weights, biases):\n",
    "    \n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden])\n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "    \n",
    "    results = tf.matmul(final_state[1], weights['out']) + biases['out']\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = RNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-0e458b1901e3>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = one_hot_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples-78, batch_size):\n",
    "        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_train = len(x_train)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training ......')\n",
    "    print()\n",
    "    max_acc = 0\n",
    "    \n",
    "    for i in range(epoches):\n",
    "        X_train,y_train = shuffle(x_train,y_train)\n",
    "        for offset in range(0,n_train,batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_train[offset:end],y_train[offset:end]\n",
    "#             size = len(batch_x)\n",
    "#             batch_x = batch_x.reshape([size, n_inputs])\n",
    "            if(len(batch_x) != batch_size): continue\n",
    "            sess.run(train_op,feed_dict = {x:batch_x, y:batch_y})\n",
    "            \n",
    "        \n",
    "        validation_accuracy = evaluate(x_test, y_test)\n",
    "        \n",
    "        if(max_acc < validation_accuracy):\n",
    "            max_acc = validation_accuracy\n",
    "            saver.save(sess, './result')\n",
    "            print(\"Model saved with accuracy:\", max_acc)\n",
    "        if i%5 == 0:\n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "#             print('Can be better, continue training...')\n",
    "            print()\n",
    "\n",
    "    print(\"maximal accuracy is:\", max_acc)\n",
    "    print(\"End training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
